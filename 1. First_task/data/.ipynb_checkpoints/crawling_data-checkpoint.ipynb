{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import time\n",
    " \n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-20 03:01:48.924063\n",
      "2020-02-20 03:14:54.324325\n"
     ]
    }
   ],
   "source": [
    "## 카카오\n",
    "stockItem = '035720'\n",
    " \n",
    "url = 'http://finance.naver.com/item/sise_day.nhn?code='+ stockItem\n",
    "html = urlopen(url) \n",
    "source = BeautifulSoup(html.read(), \"html.parser\")\n",
    " \n",
    "maxPage=source.find_all(\"table\",align=\"center\")\n",
    "mp = maxPage[0].find_all(\"td\",class_=\"pgRR\")\n",
    "mpNum = int(mp[0].a.get('href')[-3:])\n",
    "\n",
    "company_list = [] \n",
    "date_list = []\n",
    "closing_price_list = []\n",
    "\n",
    "print(datetime.datetime.now())\n",
    "for page in range(1, mpNum+1):\n",
    "    url = 'http://finance.naver.com/item/sise_day.nhn?code=' + stockItem +'&page='+ str(page)\n",
    "    html = urlopen(url)\n",
    "    source = BeautifulSoup(html.read(), \"html.parser\")\n",
    "    srlists=source.find_all(\"tr\")\n",
    "    isCheckNone = None\n",
    "\n",
    "    if((page % 1) == 0):\n",
    "        time.sleep(1.50)\n",
    "\n",
    "    for i in range(1,len(srlists)-1):\n",
    "        if(srlists[i].span != isCheckNone):\n",
    "            company_list.append(\"카카오\")\n",
    "            date_list.append(srlists[i].find_all('td',align='center')[0].text)\n",
    "            closing_price_list.append(srlists[i].find_all('td',class_='num')[0].text)\n",
    "            \n",
    "df_kakao = pd.DataFrame(list(zip(company_list,date_list,closing_price_list)), columns=['company','date','closing_price'])\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-20 03:14:54.389571\n",
      "2020-02-20 03:30:37.953548\n"
     ]
    }
   ],
   "source": [
    "## 삼성전자\n",
    "stockItem = '005930'\n",
    " \n",
    "url = 'http://finance.naver.com/item/sise_day.nhn?code='+ stockItem\n",
    "html = urlopen(url) \n",
    "source = BeautifulSoup(html.read(), \"html.parser\")\n",
    " \n",
    "maxPage=source.find_all(\"table\",align=\"center\")\n",
    "mp = maxPage[0].find_all(\"td\",class_=\"pgRR\")\n",
    "mpNum = int(mp[0].a.get('href')[-3:])\n",
    "\n",
    "company_list = [] \n",
    "date_list = []\n",
    "closing_price_list = []\n",
    "\n",
    "print(datetime.datetime.now())\n",
    "for page in range(1, mpNum+1):\n",
    "    url = 'http://finance.naver.com/item/sise_day.nhn?code=' + stockItem +'&page='+ str(page)\n",
    "    html = urlopen(url)\n",
    "    source = BeautifulSoup(html.read(), \"html.parser\")\n",
    "    srlists=source.find_all(\"tr\")\n",
    "    isCheckNone = None\n",
    "\n",
    "    if((page % 1) == 0):\n",
    "        time.sleep(1.50)\n",
    "\n",
    "    for i in range(1,len(srlists)-1):\n",
    "        if(srlists[i].span != isCheckNone):\n",
    "            company_list.append(\"삼성전자\")\n",
    "            date_list.append(srlists[i].find_all('td',align='center')[0].text)\n",
    "            closing_price_list.append(srlists[i].find_all('td',class_='num')[0].text)\n",
    "\n",
    "df_samsung = pd.DataFrame(list(zip(company_list,date_list,closing_price_list)), columns=['company','date','closing_price'])\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-20 03:30:38.012546\n",
      "2020-02-20 03:36:19.034037\n"
     ]
    }
   ],
   "source": [
    "## KMH\n",
    "stockItem = '122450'\n",
    " \n",
    "url = 'http://finance.naver.com/item/sise_day.nhn?code='+ stockItem\n",
    "html = urlopen(url) \n",
    "source = BeautifulSoup(html.read(), \"html.parser\")\n",
    " \n",
    "maxPage=source.find_all(\"table\",align=\"center\")\n",
    "mp = maxPage[0].find_all(\"td\",class_=\"pgRR\")\n",
    "mpNum = int(mp[0].a.get('href')[-3:])\n",
    "\n",
    "company_list = [] \n",
    "date_list = []\n",
    "closing_price_list = []\n",
    "\n",
    "print(datetime.datetime.now())\n",
    "for page in range(1, mpNum+1):\n",
    "    url = 'http://finance.naver.com/item/sise_day.nhn?code=' + stockItem +'&page='+ str(page)\n",
    "    html = urlopen(url)\n",
    "    source = BeautifulSoup(html.read(), \"html.parser\")\n",
    "    srlists=source.find_all(\"tr\")\n",
    "    isCheckNone = None\n",
    "\n",
    "    if((page % 1) == 0):\n",
    "        time.sleep(1.50)\n",
    "\n",
    "    for i in range(1,len(srlists)-1):\n",
    "        if(srlists[i].span != isCheckNone):\n",
    "            company_list.append(\"KMH\")\n",
    "            date_list.append(srlists[i].find_all('td',align='center')[0].text)\n",
    "            closing_price_list.append(srlists[i].find_all('td',class_='num')[0].text)\n",
    "\n",
    "df_KMH = pd.DataFrame(list(zip(company_list,date_list,closing_price_list)), columns=['company','date','closing_price'])\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-20 09:44:20.734563\n",
      "2020-02-20 09:46:41.835107\n"
     ]
    }
   ],
   "source": [
    "## 에스티팜\n",
    "stockItem = '237690'\n",
    " \n",
    "url = 'http://finance.naver.com/item/sise_day.nhn?code='+ stockItem\n",
    "html = urlopen(url) \n",
    "source = BeautifulSoup(html.read(), \"html.parser\")\n",
    " \n",
    "maxPage=source.find_all(\"table\",align=\"center\")\n",
    "mp = maxPage[0].find_all(\"td\",class_=\"pgRR\")\n",
    "mpNum = int(mp[0].a.get('href')[-2:])\n",
    "\n",
    "company_list = [] \n",
    "date_list = []\n",
    "closing_price_list = []\n",
    "\n",
    "print(datetime.datetime.now())\n",
    "for page in range(1, mpNum+1):\n",
    "    url = 'http://finance.naver.com/item/sise_day.nhn?code=' + stockItem +'&page='+ str(page)\n",
    "    html = urlopen(url)\n",
    "    source = BeautifulSoup(html.read(), \"html.parser\")\n",
    "    srlists=source.find_all(\"tr\")\n",
    "    isCheckNone = None\n",
    "\n",
    "    if((page % 1) == 0):\n",
    "        time.sleep(1.50)\n",
    "\n",
    "    for i in range(1,len(srlists)-1):\n",
    "        if(srlists[i].span != isCheckNone):\n",
    "            company_list.append(\"에스티팜\")\n",
    "            date_list.append(srlists[i].find_all('td',align='center')[0].text)\n",
    "            closing_price_list.append(srlists[i].find_all('td',class_='num')[0].text)\n",
    "\n",
    "df_stpharm = pd.DataFrame(list(zip(company_list,date_list,closing_price_list)), columns=['company','date','closing_price'])\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-20 09:46:41.928669\n",
      "2020-02-20 09:52:36.343723\n"
     ]
    }
   ],
   "source": [
    "## KTis\n",
    "stockItem = '058860'\n",
    " \n",
    "url = 'http://finance.naver.com/item/sise_day.nhn?code='+ stockItem\n",
    "html = urlopen(url) \n",
    "source = BeautifulSoup(html.read(), \"html.parser\")\n",
    " \n",
    "maxPage=source.find_all(\"table\",align=\"center\")\n",
    "mp = maxPage[0].find_all(\"td\",class_=\"pgRR\")\n",
    "mpNum = int(mp[0].a.get('href')[-3:])\n",
    "\n",
    "company_list = [] \n",
    "date_list = []\n",
    "closing_price_list = []\n",
    "\n",
    "print(datetime.datetime.now())\n",
    "for page in range(1, mpNum+1):\n",
    "    url = 'http://finance.naver.com/item/sise_day.nhn?code=' + stockItem +'&page='+ str(page)\n",
    "    html = urlopen(url)\n",
    "    source = BeautifulSoup(html.read(), \"html.parser\")\n",
    "    srlists=source.find_all(\"tr\")\n",
    "    isCheckNone = None\n",
    "\n",
    "    if((page % 1) == 0):\n",
    "        time.sleep(1.50)\n",
    "\n",
    "    for i in range(1,len(srlists)-1):\n",
    "        if(srlists[i].span != isCheckNone):\n",
    "            company_list.append(\"KTis\")\n",
    "            date_list.append(srlists[i].find_all('td',align='center')[0].text)\n",
    "            closing_price_list.append(srlists[i].find_all('td',class_='num')[0].text)\n",
    "\n",
    "df_KTis = pd.DataFrame(list(zip(company_list,date_list,closing_price_list)), columns=['company','date','closing_price'])\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.concat([df_kakao, df_samsung, df_KMH, df_stpharm, df_KTis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company</th>\n",
       "      <th>date</th>\n",
       "      <th>closing_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>카카오</td>\n",
       "      <td>2020.02.19</td>\n",
       "      <td>190,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>카카오</td>\n",
       "      <td>2020.02.18</td>\n",
       "      <td>182,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>카카오</td>\n",
       "      <td>2020.02.17</td>\n",
       "      <td>180,500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>카카오</td>\n",
       "      <td>2020.02.14</td>\n",
       "      <td>180,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>카카오</td>\n",
       "      <td>2020.02.13</td>\n",
       "      <td>179,000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  company        date closing_price\n",
       "0     카카오  2020.02.19       190,000\n",
       "1     카카오  2020.02.18       182,000\n",
       "2     카카오  2020.02.17       180,500\n",
       "3     카카오  2020.02.14       180,000\n",
       "4     카카오  2020.02.13       179,000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df.to_csv('stock_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_3",
   "language": "python",
   "name": "python_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
